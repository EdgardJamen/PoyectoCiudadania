<!doctype html>
<html lang="es">
<head>
<link rel="stylesheet" type="text/css" href="base.css" />
<link rel="stylesheet" type="text/css" href="content.css" />
<link rel="stylesheet" type="text/css" href="nav.css" />
<meta http-equiv="content-type" content="text/html;  charset=utf-8" />
<title>Casos de discriminación | ProyectoFinalCiudadania1MF </title>
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<meta name="author" content="Edgard Jamen" />
<link rel="license" type="text/html" href="http://creativecommons.org/licenses/by-sa/4.0/" />
<meta name="generator" content="eXeLearning 2.9 - exelearning.net" />
<!--[if lt IE 9]><script type="text/javascript" src="exe_html5.js"></script><![endif]-->
<script type="text/javascript" src="exe_jquery.js"></script>
<script type="text/javascript" src="common_i18n.js"></script>
<script type="text/javascript" src="common.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
</head>
<body class="exe-web-site" id="exe-node-4"><script type="text/javascript">document.body.className+=" js"</script>
<div id="content">
<p id="skipNav"><a href="#main" class="sr-av">Saltar la navegación</a></p>
<header id="header" ><div id="headerContent">ProyectoFinalCiudadania1MF</div></header>
<nav id="siteNav">
<ul>
   <li><a href="index.html" class="daddy main-node">Explorando el futuro inteligente</a></li>
   <li class="current-page-parent"><a href="inteligencia_artificial_en_la_vida_real.html" class="current-page-parent daddy">Inteligencia artificial en la vida real</a>
   <ul>
      <li class="current-page-parent"><a href="impactos_de_la_ia.html" class="current-page-parent daddy">Impactos de la IA</a>
      <ul>
         <li id="active"><a href="casos_de_discriminacin.html" class="active daddy">Casos de discriminación</a>
         <ul>
            <li><a href="obra_de_arte_con_ia.html" class="daddy">Obra de arte con IA</a>
            <ul class="other-section">
               <li><a href="ia_amiga.html" class="daddy">IA amiga</a>
               <ul class="other-section">
                  <li><a href="tendencias_futuras_con_ia.html" class="daddy">Tendencias Futuras con IA</a>
                  <ul class="other-section">
                     <li><a href="ensayo.html" class="daddy">Ensayo</a>
                     <ul class="other-section">
                        <li><a href="nos_divertimos_un_poco.html" class="no-ch">Nos divertimos un poco?</a></li>
                        <li><a href="proyecto_final_ciudadania.html" class="no-ch">Proyecto final Ciudadania</a></li>
                     </ul>
                     </li>
                  </ul>
                  </li>
               </ul>
               </li>
            </ul>
            </li>
         </ul>
         </li>
      </ul>
      </li>
   </ul>
   </li>
</ul>
</nav>
<div id='topPagination'>
<nav class="pagination noprt">
<a href="impactos_de_la_ia.html" class="prev"><span><span>&laquo; </span>Anterior</span></a> <span class="sep">| </span><a href="obra_de_arte_con_ia.html" class="next"><span>Siguiente<span> &raquo;</span></span></a>
</nav>
</div>
<div id="main-wrapper">
<section id="main">
<header id="nodeDecoration"><h1 id="nodeTitle">Casos de discriminación</h1></header>
<article class="iDevice_wrapper textIdevice em_iDevice em_iDevice_english" id="id3">
<div class="iDevice emphasis1" >
<header class="iDevice_header" style="background-image:url(icon_english.png)"><h1 class="iDeviceTitle">Sistema de salud en EEUU</h1></header>
<div class="iDevice_inner">
<div class="iDevice_content_wrapper">
<div id="ta3_130_2" class="block iDevice_content">
<div class="exe-text"><p>El problema fue descubierto por un equipo de investigadores liderado por Ziad Obermeyer de la Universidad de California, Berkeley. Publicaron sus hallazgos en la revista Science en octubre de 2019. El estudio reveló que un algoritmo utilizado en hospitales para predecir qué pacientes necesitarían atención médica adicional estaba favoreciendo a los pacientes blancos sobre los negros. Aunque la raza no era una variable directa en el algoritmo, el historial de costos de atención médica, que estaba correlacionado con la raza, causó esta discriminación.</p>
<p><strong>¿Cómo se descubrió?<br /></strong>Los investigadores analizaron los datos de millones de pacientes y encontraron que los pacientes negros, a pesar de tener condiciones de salud más graves, eran menos propensos a ser referidos a programas de atención médica adicional en comparación con los pacientes blancos. Esto se debía a que el algoritmo utilizaba los costos de atención médica como una medida de la necesidad de atención futura. Dado que los pacientes negros históricamente han tenido menos acceso a atención médica de alta calidad, sus costos eran más bajos, lo que llevó al algoritmo a subestimar sus necesidades médicas.</p>
<p><strong>Consecuencias del descubrimiento</strong><br />El descubrimiento tuvo varias consecuencias importantes:</p>
<p><span style="background-color: #ffffff;"><span style="background-color: #ccffcc;">Investigaciones regulatorias:</span> </span>El organismo regulador de seguros del estado de Nueva York inició una investigación sobre la empresa UnitedHealth Group, que utilizaba este algoritmo.</p>
<p><span style="background-color: #ccffcc;">Revisión de algoritmos: </span>Se instó a las empresas y organizaciones a revisar y ajustar sus algoritmos para evitar sesgos similares en el futuro.</p>
<p><span style="background-color: #ccffcc;">Conciencia sobre el sesgo en IA:</span> El caso aumentó la conciencia sobre cómo los algoritmos pueden perpetuar y amplificar las desigualdades existentes si no se diseñan y supervisan adecuadamente.</p>
<p><strong>Parte de la programación que causó la discriminación.</strong><br />El sesgo en el algoritmo se debió a la elección de la variable de costos de atención médica como un proxy para la necesidad de atención futura. Esta variable estaba correlacionada con la raza debido a las disparidades históricas en el acceso a la atención médica. Al utilizar los costos en lugar de medidas directas de la salud del paciente, el algoritmo terminó favoreciendo a los pacientes blancos.</p>
<p><strong>Relevancia del caso.</strong><br />Este caso es un ejemplo claro de cómo los sesgos en los datos pueden influir en los resultados de los algoritmos de IA y la importancia de diseñar sistemas que sean justos y equitativos. Destaca la necesidad de una supervisión continua y ajustes en los algoritmos para asegurar que no perpetúen las desigualdades existentes.</p></div>
</div>
</div>
</div>
</div>
</article>
<div id="packageLicense" class="cc cc-by-sa">
<p><span>Obra publicada con</span> <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Licencia Creative Commons Reconocimiento Compartir igual 4.0</a></p>
</div>
</section>
</div>
<div id='bottomPagination'>
<nav class="pagination noprt">
<a href="impactos_de_la_ia.html" class="prev"><span><span>&laquo; </span>Anterior</span></a> <span class="sep">| </span><a href="obra_de_arte_con_ia.html" class="next"><span>Siguiente<span> &raquo;</span></span></a>
</nav>
</div>
</div>
<p id="made-with-eXe"><a href="https://exelearning.net/" target="_blank" rel="noopener"><span>Creado con eXeLearning<span> (Ventana nueva)</span></span></a></p><script type="text/javascript" src="_style_js.js"></script></body></html>